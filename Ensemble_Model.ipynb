{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Ensemble_Model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D18JwKZLLGPR","executionInfo":{"status":"ok","timestamp":1615032095673,"user_tz":-120,"elapsed":80303,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}},"outputId":"20e544bd-2b9f-4d41-820b-a05a16f3c199"},"source":["import sys\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","# FOLDERNAME = 'ex3_302168687_200409159'\n","sys.path.append('/content/drive/MyDrive')\n","# sys.path.append('/content/drive/My Drive/{}/Q4'.format(FOLDERNAME))\n","%cd /content/drive/MyDrive\n","time_test=True\n","# %load_ext autoreload\n","# %autoreload 2"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive/\n","/content/drive/MyDrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1_RT8Ynill-y"},"source":["Imports"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X8O05y5flnw5","executionInfo":{"status":"ok","timestamp":1615032266448,"user_tz":-120,"elapsed":178319,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}},"outputId":"187f1f31-d27d-4411-d2bb-6d1a35c88f2e"},"source":["!pip install torch # ==1.7.0+cu101\n","!pip install torchaudio==0.7.0\n","!pip install torchvision==0.8.0\n","!pip install soundfile"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.7.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n","Collecting torchaudio==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/75/5ce994c76cf7b53ff8c577d7a8221fa0c9dfe9e34c0536c6eaf3e466788a/torchaudio-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 4.5MB/s \n","\u001b[?25hCollecting torch==1.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/74/d52c014fbfb50aefc084d2bf5ffaa0a8456f69c586782b59f93ef45e2da9/torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7MB)\n","\u001b[K     |████████████████████████████████| 776.8MB 20kB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (1.19.5)\n","Collecting dataclasses\n","  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchaudio==0.7.0) (0.16.0)\n","\u001b[31mERROR: torchvision 0.8.2+cu101 has requirement torch==1.7.1, but you'll have torch 1.7.0 which is incompatible.\u001b[0m\n","Installing collected packages: dataclasses, torch, torchaudio\n","  Found existing installation: torch 1.7.1+cu101\n","    Uninstalling torch-1.7.1+cu101:\n","      Successfully uninstalled torch-1.7.1+cu101\n","Successfully installed dataclasses-0.6 torch-1.7.0 torchaudio-0.7.0\n","Collecting torchvision==0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/3f/4f45249458a0dee85bff7acf4a2ac6177708253f1f318fcf6ee230fb864f/torchvision-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (11.8MB)\n","\u001b[K     |████████████████████████████████| 11.8MB 130kB/s \n","\u001b[?25hRequirement already satisfied: torch==1.7.0 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (1.7.0)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (7.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (1.19.5)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7.0->torchvision==0.8.0) (0.16.0)\n","Installing collected packages: torchvision\n","  Found existing installation: torchvision 0.8.2+cu101\n","    Uninstalling torchvision-0.8.2+cu101:\n","      Successfully uninstalled torchvision-0.8.2+cu101\n","Successfully installed torchvision-0.8.0\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.5)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iq4WtlpglqY7","executionInfo":{"status":"ok","timestamp":1615032270037,"user_tz":-120,"elapsed":179927,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}},"outputId":"93118c23-cae7-4219-ca59-0a6eead43bac"},"source":["import torch.nn as nn\n","import torch\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils import data\n","from torchvision import datasets, models, transforms\n","import torchaudio\n","\n","import os\n","import matplotlib\n","import pylab\n","import librosa\n","import numpy as np\n","\n","import soundfile as sf\n","\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","import librosa.display\n","\n","#!apt-get install sox libsox-dev libsox-fmt-all\n","#!pip install git+git://github.com/pytorch/audio\n","from IPython.display import Audio\n","\n","from scipy.io import wavfile\n","from librosa.feature import mfcc\n","# Load the Pandas libraries with alias 'pd' \n","import pandas as pd \n","\n","import os\n","import pandas as pd\n","import random\n","\n","from pathlib import Path"],"execution_count":3,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n","  '\"sox\" backend is being deprecated. '\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"lQYxuIw6mivO"},"source":["Model architectures"]},{"cell_type":"code","metadata":{"id":"fUXngPobDniy","executionInfo":{"status":"ok","timestamp":1615032270604,"user_tz":-120,"elapsed":177860,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}}},"source":["class MonoCnnAudioNet(nn.Module):\n","    def __init__(self,numClasses):\n","        super(MonoCnnAudioNet,self).__init__()\n","        self.num_classes = numClasses\n","        self.fc_features = 128\n","        self.C1 = nn.Conv2d(1,32,3,padding=1)\n","        self.C11 = nn.Conv2d(32,32,3,padding=1)\n","        self.C2 = nn.Conv2d(32,64,3,padding=1)\n","        self.C22 = nn.Conv2d(64,64,3,padding=1)\n","        self.C3 = nn.Conv2d(64,128,3,padding=1)\n","        self.C33 = nn.Conv2d(128,128,3,padding=1)\n","        \n","        self.BN1 = nn.BatchNorm2d(32)\n","        self.BN11 = nn.BatchNorm2d(32)\n","        self.BN2 = nn.BatchNorm2d(64)\n","        self.BN22 = nn.BatchNorm2d(64)\n","        self.BN3 = nn.BatchNorm2d(128)\n","        self.BN33 = nn.BatchNorm2d(128)\n","        \n","        self.BNFC = nn.BatchNorm1d(self.fc_features)\n","        self.maxpool1 = nn.MaxPool2d(2,2)\n","        self.maxpool2 = nn.MaxPool2d((2,2),(2,2))\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        self.flatten = torch.nn.Flatten()\n","\n","        # self.fc1 = nn.Linear(64*32*32,self.fc_features)\n","        self.fc1 = nn.Linear(512,self.fc_features)\n","        self.fc2 = nn.Linear(self.fc_features, 1) \n","        \n","        self.dropout = nn.Dropout(0.35)\n","\n","        self.BNout = nn.BatchNorm1d(1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","        \n","    def forward(self,x,extra_feature):\n","        # add sequence of convolutional and max pooling layers\n","        x = self.maxpool1(F.relu(self.BN1(self.C1(x))))\n","        x = self.maxpool1(F.relu(self.BN11(self.C11(x)))) ##\n","        x = self.dropout(x)                                 ## @@\n","        x = self.maxpool1(F.relu(self.BN2(self.C2(x))))     \n","        x = self.maxpool1(F.relu(self.BN22(self.C22(x)))) ##\n","        # x = F.relu(self.BN3(self.C3(x)))                  ##\n","        # x = self.dropout(x)                                 ## @@  \n","        x = self.maxpool1(F.relu(self.BN3(self.C3(x))))     ## @@\n","        x = self.maxpool1(F.relu(self.BN33(self.C33(x)))) ##\n","\n","        # flatten image input\n","        # x = x.view(-1,64*32*32)\n","        x = self.flatten(x)\n","\n","        x = self.BNFC(self.fc1(x))\n","        x = self.dropout(x)\n","        if not time_test:\n","          x = torch.cat((x, extra_feature/240), 1) #normalize extra_feature\n","          x = self.fc2(x)\n","        else:\n","          # x = torch.cat((x, (extra_feature-2)/100), 1) #normalize extra_feature\n","          x = self.fc2(x)\n","          x = self.BNout(x)\n","          x = self.sigmoid(x) * 1440\n","          # x = (1+self.tanh(x))/2*1440\n","        #x = self.softmax(x) #already included in CrossEntropyLoss\n","        return x\n","        \n","class EncodedCnnAudioNet(nn.Module):\n","    def __init__(self,numClasses):\n","        super(EncodedCnnAudioNet,self).__init__()\n","        self.num_classes = numClasses\n","        self.fc_features = 128\n","        self.C1 = nn.Conv2d(3,32,3,padding=1)\n","        self.C11 = nn.Conv2d(32,32,3,padding=1)\n","        self.C2 = nn.Conv2d(32,64,3,padding=1)\n","        self.C22 = nn.Conv2d(64,64,3,padding=1)\n","        self.C3 = nn.Conv2d(64,128,3,padding=1)\n","        self.C33 = nn.Conv2d(128,128,3,padding=1)\n","        \n","        self.BN1 = nn.BatchNorm2d(32)\n","        self.BN11 = nn.BatchNorm2d(32)\n","        self.BN2 = nn.BatchNorm2d(64)\n","        self.BN22 = nn.BatchNorm2d(64)\n","        self.BN3 = nn.BatchNorm2d(128)\n","        self.BN33 = nn.BatchNorm2d(128)\n","\n","        self.BNFC = nn.BatchNorm1d(self.fc_features)\n","        self.maxpool1 = nn.MaxPool2d(2,2)\n","        self.maxpool2 = nn.MaxPool2d((2,2),(2,2))\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        self.flatten = torch.nn.Flatten()\n","        \n","        # self.fc1 = nn.Linear(64*32*32,self.fc_features)\n","        # self.fc2 = nn.Linear(self.fc_features + 1,self.num_classes) \n","        self.fc1 = nn.Linear(512,self.fc_features)\n","        self.fc2 = nn.Linear(self.fc_features, 1) \n","        self.dropout = nn.Dropout(0.35)\n","\n","        self.BNout = nn.BatchNorm1d(1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","        \n","    def forward(self,x,extra_feature):\n","        # add sequence of convolutional and max pooling layers\n","        x = self.maxpool1(F.relu(self.BN1(self.C1(x))))\n","        x = self.maxpool1(F.relu(self.BN11(self.C11(x))))\n","        x = self.maxpool1(F.relu(self.BN2(self.C2(x))))\n","        x = self.maxpool1(F.relu(self.BN22(self.C22(x))))\n","        x = self.maxpool1(F.relu(self.BN3(self.C3(x))))\n","        x = self.maxpool1(F.relu(self.BN33(self.C33(x))))\n","        # flatten image input\n","        x = self.flatten(x)\n","        x =  self.BNFC(self.fc1(x))\n","        x = self.dropout(x)\n","        if not time_test:\n","          x = torch.cat((x, extra_feature/240), 1) #normalize extra_feature\n","          x = self.fc2(x)\n","        else:\n","          # x = torch.cat((x, (extra_feature-2)/100), 1) #normalize extra_feature\n","          x = self.fc2(x)\n","          x = self.BNout(x)\n","          x = self.sigmoid(x) * 1440\n","        #x = self.softmax(x) #already included in CrossEntropyLoss\n","        return x\n","\n","class HPSSCnnAudioNet(nn.Module):\n","    def __init__(self,numClasses):\n","        super(HPSSCnnAudioNet,self).__init__()\n","        self.num_classes = numClasses\n","        self.fc_features = 64\n","        self.C1 = nn.Conv2d(2,32,3,padding=1)\n","        self.C11 = nn.Conv2d(32,32,3,padding=1)\n","        self.C2 = nn.Conv2d(32,64,3,padding=1)\n","        self.C22 = nn.Conv2d(64,64,3,padding=1)\n","        self.C3 = nn.Conv2d(64,128,3,padding=1)\n","        self.C33 = nn.Conv2d(128,128,3,padding=1)\n","        \n","        self.BN1 = nn.BatchNorm2d(32)\n","        self.BN11 = nn.BatchNorm2d(32)\n","        self.BN2 = nn.BatchNorm2d(64)\n","        self.BN22 = nn.BatchNorm2d(64)\n","        self.BN3 = nn.BatchNorm2d(128)\n","        self.BN33 = nn.BatchNorm2d(128)\n","\n","        self.BNFC = nn.BatchNorm1d(self.fc_features)\n","        self.maxpool1 = nn.MaxPool2d(2,2)\n","        self.maxpool2 = nn.MaxPool2d((2,2),(2,2))\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","        self.flatten = torch.nn.Flatten()\n","        \n","        # self.fc1 = nn.Linear(64*32*32,self.fc_features)\n","        # self.fc2 = nn.Linear(self.fc_features + 1,self.num_classes) \n","        self.fc1 = nn.Linear(1152,self.fc_features)\n","        self.fc2 = nn.Linear(self.fc_features, 1) \n","        self.dropout = nn.Dropout(0.35)\n","\n","        self.BNout = nn.BatchNorm1d(1)\n","        self.sigmoid = nn.Sigmoid()\n","        self.tanh = nn.Tanh()\n","  \n","    def forward(self,x,extra_feature):\n","        # add sequence of convolutional and max pooling layers\n","        x = self.maxpool1(F.relu(self.BN1(self.C1(x))))\n","        x = self.maxpool1(F.relu(self.BN11(self.C11(x))))\n","        x = self.maxpool1(F.relu(self.BN2(self.C2(x))))\n","        x = self.maxpool1(F.relu(self.BN22(self.C22(x))))\n","        x = self.maxpool1(F.relu(self.BN3(self.C3(x))))\n","        x = self.maxpool1(F.relu(self.BN33(self.C33(x))))\n","        # flatten image input\n","        x = x.view(-1,1152)\n","        x =  self.BNFC(self.fc1(x))\n","        x = self.dropout(x)\n","        if not time_test:\n","          x = torch.cat((x, extra_feature/240), 1) #normalize extra_feature\n","          x = self.fc2(x)\n","        else:\n","          # x = torch.cat((x, (extra_feature-2)/100), 1) #normalize extra_feature\n","          x = self.fc2(x)\n","          x = self.BNout(x)\n","          x = self.sigmoid(x) * 1440\n","        #x = self.softmax(x) #already included in CrossEntropyLoss\n","        return x\n","        \n","        \n","class EnsembleCnnAudioNet(nn.Module):\n","    def __init__(self, numClasses, mono_model, mono_acc, mono_weights,\n","                 encoded_model, encoded_acc, encoded_weights, \n","                 hpss_model, hpss_acc, hpss_weights):\n","        super(EnsembleCnnAudioNet,self).__init__()\n","        self.num_classes = numClasses\n","        self.mono_model = mono_model\n","        self.mono_weights = mono_weights\n","        self.mono_acc = mono_acc\n","        self.encoded_model = encoded_model\n","        self.encoded_weights = encoded_weights\n","        self.encoded_acc = encoded_acc\n","        self.hpss_model = hpss_model\n","        self.hpss_weights = hpss_weights\n","        self.hpss_acc = hpss_acc\n","        self.alpha = 2\n","\n","        \n","    def forward(self, mono, encoded, hpss, timestamp):\n","        output_mono = self.mono_model(mono,timestamp)\n","        \n","        output_encoded = self.encoded_model(encoded,timestamp)\n","        \n","        output_hpss = self.hpss_model(hpss,timestamp)\n","        \n","        # outP = torch.pow(mono_acc, 3) * (outP_mono * mono_weights) + \\\n","        #         torch.pow(encoded_acc, 3) * (outP_encoded * encoded_weights) + \\\n","        #         torch.pow(hpss_acc,  3) * (outP_hpss * hpss_weights)\n","        pow = 4\n","        outP = (torch.pow(mono_acc, pow) * (output_mono) + \\\n","                torch.pow(encoded_acc, pow) * (output_encoded) + \\\n","                torch.pow(hpss_acc,  pow) * (output_hpss))/(torch.pow(mono_acc, pow) + torch.pow(encoded_acc, pow) + torch.pow(hpss_acc,  pow))\n","        \n","        return outP\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RSmajccjzj9"},"source":["Utility functions"]},{"cell_type":"code","metadata":{"id":"igH4M0YMj34g","executionInfo":{"status":"ok","timestamp":1615032270605,"user_tz":-120,"elapsed":169724,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}}},"source":["def loadModel(model, model_name, path):\n","    path = F\"{path}/{model_name}.pt\"\n","    model.load_state_dict(torch.load(path))\n","    return model\n","\n","@torch.no_grad()\n","def calculateTestLoss(model, criterion, test_generator):\n","    model.eval()\n","    test_loss = 0\n","    Label_all = []\n","    Output_all = []\n","    for dataBatch,target,timestamp in test_generator:\n","        dataBatch, target, timestamp = dataBatch.float().cuda(), target.cuda(), timestamp.unsqueeze(1).cuda()\n","        output = model(dataBatch, timestamp)\n","        if not time_test:\n","          loss = criterion(output,torch.max(target.long(), 1)[1])\n","        else:\n","          # loss = min(criterion(output, target), criterion(output-1, target), criterion(output+1, target))\n","          loss = min(criterion(output, target), criterion(output-1440, target), criterion(output+1440, target))\n","        test_loss += loss.item()*dataBatch.size(0)\n","        Label_all.extend(target.tolist())\n","        Output_all.extend(output.tolist())\n","\n","    x = np.abs(np.array(Label_all) - np.array(Output_all))\n","    y = 1440 - np.abs(np.array(Label_all) - np.array(Output_all))\n","    accuracy = (np.minimum(x,y)<=240).sum() / len(Label_all)\n","    # y = 1 - np.abs(np.array(Label_all) - np.array(Output_all))\n","    # accuracy = (np.minimum(x,y)<=(180/1440)).sum() / len(Label_all)\n","\n","    return test_loss/len(test_generator), accuracy"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dEfBsMIrkAHw"},"source":["Dataset"]},{"cell_type":"code","metadata":{"id":"YjP4A9lekBdH","executionInfo":{"status":"ok","timestamp":1615032270605,"user_tz":-120,"elapsed":167518,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}}},"source":["class Dataset(data.Dataset):\n","    def __init__(self, fileNames, locations, dataPath1, dataPath2, numClasses):\n","        'Initialization'\n","        self.locations = locations\n","        self.file_names = fileNames\n","        self.data_path1= dataPath1\n","        self.data_path2= dataPath2\n","        self.num_classes = numClasses\n","        \n","    def __len__(self):\n","        'Denotes the total number of samples'\n","        return len(self.file_names)\n","\n","    def __getitem__(self, index):\n","        'Generates one sample of data'\n","        file_name = self.file_names[index]\n","        location = self.locations[index]\n","        try:\n","          file_mono = os.path.join(self.data_path1, \"Mono\", str(location), str(file_name))\n","          Im_mono = torch.load(file_mono)\n","        except:\n","          file_mono = os.path.join(self.data_path2, \"Mono\", str(location), str(file_name))\n","          Im_mono = torch.load(file_mono)\n","        try:\n","          file_encoded = os.path.join(self.data_path1, \"Encoded\", str(location), str(file_name))\n","          Im_encoded = torch.load(file_encoded)\n","        except:\n","          file_encoded = os.path.join(self.data_path2, \"Encoded\", str(location), str(file_name))\n","          Im_encoded = torch.load(file_encoded)\n","        try:\n","          file_hpss = os.path.join(self.data_path1, \"HPSS\", str(location), str(file_name))\n","          Im_hpss = torch.load(file_hpss)\n","        except:\n","          file_hpss = os.path.join(self.data_path2, \"HPSS\", str(location), str(file_name))\n","          Im_hpss = torch.load(file_hpss)\n","        Im_encoded.requires_grad=False\n","        label_out = np.zeros(self.num_classes)\n","        label_out = torch.from_numpy(label_out).float()\n","        timestamp = file_name.split(\"_\", 1)[1]\n","        timestamp = int(timestamp.split(\".\", 1)[0])/1000 # quantization of 30-minutes\n","        timestamp = torch.tensor(timestamp, dtype=torch.float32)\n","        #print(timestamp, Im.shape, label)\n","        if not time_test:\n","          label = location\n","          label_out[label-2] = 1\n","          extra_feature = timestamp\n","        else:\n","          label = (int(file_name[-9:-7])*60 + int(file_name[-7:-5])) # /1440\n","          label_out[0] = label\n","          extra_feature = location\n","\n","        return Im_mono,Im_encoded,Im_hpss,label_out,extra_feature"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0sRWZtGjlFHZ"},"source":["Load dataset"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F2gjvZg7gk22","executionInfo":{"status":"ok","timestamp":1615032277629,"user_tz":-120,"elapsed":171194,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}},"outputId":"2f8d56cd-15fd-4d09-d738-1986cfac5e51"},"source":["spots_to_train = range(2,12)\n","random.seed(3)\n","num_classes = 1\n","path = '/content/drive/MyDrive/Project/Dataset'\n","path_csv = '/content/drive/MyDrive/project_new/copy/DataSet'\n","test_path_csv = '/content/drive/MyDrive/project_new/copy/TestSet'\n","test_path = '/content/drive/MyDrive/Project/Testset'\n","#Load train data\n","mono_labels = pd.read_csv(os.path.join(path_csv, \"labels.csv\"))\n","mono_test_labels = pd.read_csv(os.path.join(test_path_csv, \"labels.csv\"))\n","mono_labels = pd.concat([mono_labels, mono_test_labels])\n","sample_indexes = [mono_label in spots_to_train for mono_label in mono_labels['label']]\n","mono_labels = mono_labels[sample_indexes]\n","mono_n_samples = len(mono_labels)\n","\n","encoded_labels = pd.read_csv(os.path.join(path_csv, \"encoded_labels.csv\"))\n","encoded_test_labels = pd.read_csv(os.path.join(test_path_csv, \"encoded_labels.csv\"))\n","encoded_labels = pd.concat([encoded_labels, encoded_test_labels])\n","sample_indexes = [encoded_label in spots_to_train for encoded_label in encoded_labels['label']]\n","encoded_labels = encoded_labels[sample_indexes]\n","encoded_n_samples = len(encoded_labels)\n","\n","\n","hpss_labels = pd.read_csv(os.path.join(path_csv, \"hpss_labels.csv\"))\n","hpss_test_labels = pd.read_csv(os.path.join(test_path_csv, \"hpss_labels.csv\"))\n","hpss_labels = pd.concat([hpss_labels, hpss_test_labels])\n","sample_indexes = [hpss_label in spots_to_train for hpss_label in hpss_labels['label']]\n","hpss_labels = hpss_labels[sample_indexes]\n","hpss_n_samples = len(hpss_labels)\n","\n","n_samples = min(mono_n_samples, encoded_n_samples)\n","n_samples = min(n_samples, hpss_n_samples)\n","\n","\n","train_len = int(n_samples * 0.9)\n","valid_len = int(n_samples * 0.05)\n","test_len  = n_samples - train_len - valid_len\n","total_list = list(range(n_samples))\n","train_list = random.sample(total_list, train_len)\n","total_list = list(set(total_list)^set(train_list))\n","valid_list = random.sample(total_list, valid_len)\n","total_list = list(set(total_list)^set(valid_list))\n","test_list = random.sample(total_list, test_len)\n","mono_test_data = mono_labels.iloc[test_list]\n","mono_train_data = mono_labels.iloc[train_list]\n","mono_valid_data = mono_labels.iloc[valid_list]\n","encoded_test_data = encoded_labels.iloc[test_list]\n","encoded_train_data = encoded_labels.iloc[train_list]\n","encoded_valid_data = encoded_labels.iloc[valid_list]\n","hpss_test_data = hpss_labels.iloc[test_list]\n","hpss_train_data = hpss_labels.iloc[train_list]\n","hpss_valid_data = hpss_labels.iloc[valid_list]\n","train_data = pd.merge(hpss_train_data, encoded_train_data)\n","train_data = pd.merge(train_data, mono_train_data)\n","test_data = pd.merge(hpss_test_data, encoded_test_data)\n","test_data = pd.merge(test_data, mono_test_data)\n","valid_data = pd.merge(hpss_valid_data, encoded_valid_data)\n","valid_data = pd.merge(valid_data, mono_valid_data)\n","\n","\n","\n","training_set = Dataset(train_data[\"filename\"].tolist(), train_data[\"label\"].tolist(), path, test_path, num_classes)\n","batch_size = 32\n","train_params = {'batch_size': batch_size,\n","          'shuffle': True,\n","          'num_workers': 6}\n","training_generator = data.DataLoader(training_set, **train_params)\n","#Load validation data\n","valid_set = Dataset(valid_data[\"filename\"].tolist(), valid_data[\"label\"].tolist(), path,  test_path, num_classes)\n","batch_size = 32\n","valid_params = {'batch_size': batch_size,\n","          'shuffle': True,\n","          'num_workers': 6}\n","valid_generator = data.DataLoader(valid_set, **valid_params)\n","#Load Test data\n","test_set = Dataset(test_data[\"filename\"].tolist(), test_data[\"label\"].tolist(), path,  test_path, num_classes)\n","batch_size = 32\n","test_params = {'batch_size': batch_size,\n","          'shuffle': True,\n","          'num_workers': 6}\n","test_generator = data.DataLoader(test_set, **test_params)\n","\n","print(\"Training set contains \" + str(train_len) + \" samples\")\n","print(\"Validation set contains \" + str(valid_len) + \" samples\")\n","print(\"Test set contains \" + str(test_len) + \" samples\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Training set contains 40500 samples\n","Validation set contains 2250 samples\n","Test set contains 2250 samples\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0fXk6v5oLCRf"},"source":["Evaluate on Test set"]},{"cell_type":"code","metadata":{"id":"FEDwUXBblGnB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615032710890,"user_tz":-120,"elapsed":433242,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}},"outputId":"29d720af-b496-4c62-9423-9abf146eab4e"},"source":["train_on_gpu=torch.cuda.is_available()\n","\n","mono_acc = torch.load(\"/content/drive/MyDrive/project_new/copy/Models/mono_accuracy.pt\").cuda()\n","mono_weights = torch.load(\"/content/drive/MyDrive/project_new/copy/Models/mono_weights.pt\").cuda()\n","\n","encoded_acc = torch.load(\"/content/drive/MyDrive/project_new/copy/Models/encoded_accuracy.pt\").cuda()\n","encoded_weights = torch.load(\"/content/drive/MyDrive/project_new/copy/Models/encoded_weights.pt\").cuda()\n","\n","hpss_acc = torch.load(\"/content/drive/MyDrive/project_new/copy/Models/hpss_accuracy.pt\").cuda()\n","hpss_weights = torch.load(\"/content/drive/MyDrive/project_new/copy/Models/hpss_weights.pt\").cuda()\n","\n","if time_test: \n","  num_classes = 1\n","else:\n","  num_classes = 10\n","mono_model_name = \"MonoCnnAudioNet\"\n","mono_model = MonoCnnAudioNet(num_classes)\n","mono_model = loadModel(mono_model, mono_model_name + \"_74\", \"/content/drive/MyDrive/project_new/copy/weights/Mono/spot_\" + '_'.join(str(spot) for spot in spots_to_train))\n","mono_model = mono_model.cuda()\n","mono_model.eval()\n","\n","encoded_model_name = \"EncodedCnnAudioNet\"\n","encoded_model = EncodedCnnAudioNet(num_classes)\n","encoded_model = loadModel(encoded_model, encoded_model_name + \"_38\", \"/content/drive/MyDrive/project_new/copy/weights/Encoded/spot_\" + '_'.join(str(spot) for spot in spots_to_train))\n","encoded_model = encoded_model.cuda()\n","encoded_model.eval()\n","\n","hpss_model_name = \"HPSSCnnAudioNet\"\n","hpss_model = HPSSCnnAudioNet(num_classes)\n","hpss_model = loadModel(hpss_model, hpss_model_name + \"_30\", \"/content/drive/MyDrive/project_new/copy/weights/HPSS/spot_\" + '_'.join(str(spot) for spot in spots_to_train))\n","hpss_model = hpss_model.cuda()\n","hpss_model.eval()\n","\n","model_name= \"EnsembleCnnAudioNet\"\n","ensemble_model = EnsembleCnnAudioNet(num_classes, mono_model, mono_acc, mono_weights,\n","                                    encoded_model, encoded_acc, encoded_weights,\n","                                    hpss_model, hpss_acc, hpss_weights)\n","\n","\n","Label_all = []\n","Output_mono = []\n","Output_encoded = []\n","Output_hpss = []\n","Output_ensemble = []\n","with torch.no_grad():                   # operations inside don't track history\n","    for mono,encoded,hpss,label,timestamp in test_generator:\n","        mono,encoded,hpss,label,timestamp = mono.unsqueeze(1).cuda(), encoded.cuda(), hpss.cuda(), label.cuda(), timestamp.unsqueeze(1).cuda()#dataBatch.unsqueeze(1).float().cuda(), label.cuda()\n","\n","        #timestamp = timestamp * 0 #see the impact of timestamp metadata (very small actually)\n","        output_mono = mono_model(mono,timestamp)\n","        Output_mono.extend(output_mono.tolist())\n","      \n","        output_encoded = encoded_model(encoded,timestamp)\n","        Output_encoded.extend(output_encoded.tolist())\n","      \n","        output_hpss = hpss_model(hpss,timestamp)\n","        Output_hpss.extend(output_hpss.tolist())\n","      \n","        outP_ensemble = ensemble_model(mono,encoded,hpss,timestamp)\n","        Output_ensemble.extend(outP_ensemble.tolist())\n","        \n","        Label_all.extend(label.tolist())\n","x = np.abs(np.array(Label_all) - np.array(Output_mono))\n","y = 1440 - np.abs(np.array(Label_all) - np.array(Output_mono))\n","mono_accuracy = (np.minimum(x,y)<=240).sum() / len(Label_all)\n","x = np.abs(np.array(Label_all) - np.array(Output_encoded))\n","y = 1440 - np.abs(np.array(Label_all) - np.array(Output_encoded))\n","encoded_accuracy = (np.minimum(x,y)<=240).sum() / len(Label_all)\n","x = np.abs(np.array(Label_all) - np.array(Output_hpss))\n","y = 1440 - np.abs(np.array(Label_all) - np.array(Output_hpss))\n","hpss_accuracy = (np.minimum(x,y)<=240).sum() / len(Label_all)\n","x = np.abs(np.array(Label_all) - np.array(Output_ensemble))\n","y = 1440 - np.abs(np.array(Label_all) - np.array(Output_ensemble))\n","ensemble_accuracy = (np.minimum(x,y)<=240).sum() / len(Label_all)\n","print('Mono Testset accuracy: {}%'.format(mono_accuracy * 100))\n","print('Encoded Testset accuracy: {}%'.format(encoded_accuracy * 100))\n","print('HPSS Testset accuracy: {}%'.format(hpss_accuracy * 100))\n","print('Ensemble Testset accuracy: {}%'.format(ensemble_accuracy * 100))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Mono Testset accuracy: 82.71119842829077%\n","Encoded Testset accuracy: 83.1041257367387%\n","HPSS Testset accuracy: 71.51277013752456%\n","Ensemble Testset accuracy: 83.69351669941061%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CGvrqw1ELCRg"},"source":["Plot confusion matrix"]},{"cell_type":"code","metadata":{"id":"MbKY2DCgQnJw","executionInfo":{"status":"ok","timestamp":1615032787931,"user_tz":-120,"elapsed":965,"user":{"displayName":"Yoav Ikan","photoUrl":"","userId":"16307531789030825659"}}},"source":["@torch.no_grad()\n","def get_all_preds(model, loader):\n","    model.eval()\n","    all_preds = torch.tensor([])\n","    all_targets = torch.tensor([])\n","    for batch in loader:\n","        mono, encoded, hpss, labels, timestamp = batch\n","        mono, encoded, hpss, labels, timestamp = mono.unsqueeze(1).cuda(), encoded.cuda(), hpss.cuda(), labels.cuda(), timestamp.unsqueeze(1).cuda()#images.unsqueeze(1).float().cuda(), labels.cuda()\n","        preds = model(mono, encoded, hpss, timestamp)\n","        if time_test:\n","          preds = preds.cpu().numpy()\n","          preds = preds//60\n","          labels = labels//60\n","          labels = labels.cpu().numpy()\n","        else:\n","          preds = torch.nn.functional.softmax(preds)\n","        all_preds = torch.cat(\n","            (all_preds,  torch.from_numpy(preds))\n","            ,dim=0\n","        )\n","        all_targets = torch.cat(\n","            (all_targets,  torch.from_numpy(labels))\n","            ,dim=0\n","        )\n","    return all_preds.cpu(), all_targets.cpu()\n","\n","\n","def confusion_matrix(real_targets, train_preds):\n","    stacked = torch.stack((real_targets,train_preds),dim=1)\n","    cmt = torch.zeros(24,24, dtype=torch.int64)\n","    for p in stacked:\n","        tl, pl = p.tolist()\n","        cmt[tl, pl] = cmt[tl, pl] + 1\n","    \n","    return cmt\n","\n","\n","import itertools\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_confusion_matrix(cm, classes, normalize=True, title='Confusion matrix', cmap=plt.cm.Blues):\n","    if normalize:\n","        cm = cm.type(torch.FloatTensor) / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    #print(cm)\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    "],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDYlZP8PGKID"},"source":["#from sklearn.metrics import confusion_matrix\n","\n","with torch.no_grad():\n","    train_preds, real_targets = get_all_preds(ensemble_model, test_generator)\n","\n","cm = confusion_matrix(real_targets, train_preds)\n","plt.figure(figsize=(24,24))\n","classes = [str(i) for i in range(24)]\n","plot_confusion_matrix(cm, classes)\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLnIcUGxLCRi"},"source":["Plot accuracy throughout the day"]},{"cell_type":"code","metadata":{"id":"X4W14fJHLCRi","outputId":"21ac0af2-26cc-4b22-8c7d-4110ceb0ce00"},"source":["epoch_num = 10\n","Label_all = []\n","Output_all = []\n","timestamp_all = []\n","with torch.no_grad():                   # operations inside don't track history\n","\n","    for mono,encoded,hpss,label,timestamp in test_generator:\n","        if train_on_gpu:\n","            mono, encoded, hpss, label, timestamp = mono.unsqueeze(1).cuda(), encoded.cuda(), hpss.cuda(), label.cuda(), timestamp.unsqueeze(1).cuda()#dataBatch.unsqueeze(1).float().cuda(), label.cuda()\n","  \n","        outP = ensemble_model(mono, encoded, hpss, timestamp)\n","        \n","        Label_all.extend(label.argmax(1).tolist())\n","        Output_all.extend(outP.argmax(1).tolist())\n","        timestamp_all.extend(timestamp.tolist())\n","\n","array = (np.array(Label_all) == np.array(Output_all))\n","timestamp_all = np.array(timestamp_all)\n","\n","plt.figure(figsize=(8, 10))\n","timestamp_false = timestamp_all[np.where(array==False)]\n","counts_false, bins_false = np.histogram(timestamp_false, bins=22)\n","\n","timestamp_true = timestamp_all[np.where(array==True)]\n","counts_true, bins_true = np.histogram(timestamp_true, bins=22)\n","plt.subplot(1, 1, 1)  # 1 line, 2 rows, index nr 1 (first position in the subplot)\n","plt.hist(bins_true[:-1], bins_true, weights=counts_true/(counts_true+counts_false))\n","plt.title('Accuracy over hour in day')\n","plt.tight_layout()\n","\n","accuracy = (np.array(Label_all) == np.array(Output_all)).sum() / len(Label_all)\n","print('Test accuracy: {}%'.format(accuracy * 100))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/home/kfir/.local/lib/python3.6/site-packages/ipykernel_launcher.py:161: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/home/kfir/.local/lib/python3.6/site-packages/ipykernel_launcher.py:164: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","/home/kfir/.local/lib/python3.6/site-packages/ipykernel_launcher.py:167: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"],"name":"stderr"},{"output_type":"stream","text":["Test accuracy: 84.07974038015763%\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<matplotlib.figure.Figure at 0x7f9ea0ada1d0>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"kd5myryELCRi"},"source":[""],"execution_count":null,"outputs":[]}]}